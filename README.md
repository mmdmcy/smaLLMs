# ğŸš€ smaLLMs - AI Studio-Level Benchmarking Platform with Marathon Mode

[![MIT License](https://img.shields.io/badge/License-MIT-green.svg)](https://choosealicense.com/licenses/mit/)
[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![Local & Cloud](https://img.shields.io/badge/ğŸ”„-Ollama%20%2B%20LM%20Studio%20%2B%20Cloud-blue)](https://ollama.ai/)
[![Marathon Mode](https://img.shields.io/badge/ğŸƒâ€â™‚ï¸-Marathon%20Mode-purple)](https://github.com/mmdmcy/smaLLMs)
[![OpenAI-Level](https://img.shields.io/badge/ğŸ†-16%20AI%20Studio%20Benchmarks-red)](https://openai.com/)

> **ï¿½â€â™‚ï¸ Marathon Mode: Run overnight evaluation of ALL your local models with ALL 16 benchmarks**  
> Supporting Ollama, LM Studio, and Cloud APIs with comprehensive AI studio-level evaluation!

![smaLLMs Demo](https://img.shields.io/badge/Status-Production%20Ready-brightgreen) ![Benchmarks](https://img.shields.io/badge/Benchmarks-16-orange) ![Models](https://img.shields.io/badge/Local%20Models-23%2B-green)

## ğŸ¯ What is smaLLMs?

**smaLLMs** is the most comprehensive local and cloud LLM evaluation platform, supporting **ALL 16 benchmarks used by OpenAI, Anthropic, Google DeepMind, and xAI**. Evaluate your Ollama and LM Studio models with the same rigor as top AI labs.

### ğŸŒŸ Key Features

- **ğŸƒâ€â™‚ï¸ Marathon Mode**: Overnight evaluation of ALL local models with ALL benchmarks
- **ğŸ† 16 AI Studio Benchmarks**: Complete suite including AIME, GPQA, Codeforces, HealthBench
- **ğŸ”„ Local + Cloud**: Seamlessly works with Ollama (15 models), LM Studio (8 models), and cloud APIs
- **ğŸš€ One Command**: `python smaLLMs.py` - everything integrated into a single file
- **ğŸ’° Cost-Optimized**: Smart sampling and rate limiting for efficient evaluation
- **ğŸ“Š Beautiful Interface**: Real-time progress with color-coded results
- **ğŸ¯ Production-Ready**: Battle-tested evaluation methodology
- **ğŸ“ Organized Results**: Date-based structure with clean exports
- **âš¡ Windows Compatible**: Full Unicode support and robust error handling

## ğŸ”¥ NEW: Marathon Mode + 16 AI Studio Benchmarks

### ğŸƒâ€â™‚ï¸ **Marathon Mode**
Run overnight comprehensive evaluation of ALL your models:
- **Auto-Discovery**: Finds all 23+ local models (Ollama + LM Studio)
- **Smart Selection**: Choose specific models or run ALL discovered models
- **Benchmark Suites**: 18 different benchmark combinations to choose from
- **Progress Tracking**: Real-time updates and resume capability
- **Organized Results**: Clean date/time-based result organization
- **Windows Compatible**: Full Unicode support and robust timeout handling

### ğŸ† **16 AI Studio Benchmarks**
Complete benchmark suite matching major AI companies:

#### **Competition & Expert Level**
- **AIME 2024/2025**: American Invitational Mathematics Examination (o3/o4 level)
- **GPQA Diamond**: PhD-level science questions (Google-Proof Q&A) 
- **Codeforces**: Competitive programming with ELO ratings
- **HLE**: Humanity's Last Exam - Expert cross-domain evaluation
- **HealthBench**: Medical conversation safety (includes Hard variant)
- **TauBench**: Function calling and tool use evaluation

#### **Core Academic Standards**
- **GSM8K**: Grade school mathematics reasoning
- **MMLU**: Massive multitask language understanding
- **MATH**: Mathematical reasoning and competition problems
- **HumanEval**: Code generation and programming capabilities
- **ARC**: Abstract reasoning challenge
- **HellaSwag**: Commonsense reasoning

#### **Advanced Reasoning**
- **WinoGrande**: Winograd schema challenge
- **BoolQ**: Boolean question answering
- **OpenBookQA**: Multi-step reasoning with facts
- **PIQA**: Physical interaction question answering

### ğŸ¯ **18 Benchmark Suites Available**
- **Individual Benchmarks**: Any single benchmark (16 options)
- **OpenAI Suite**: Complete o3/o4 benchmark set  
- **Competition Suite**: AIME + Codeforces + MATH
- **Expert Suite**: GPQA + HLE + HealthBench
- **Academic Suite**: MMLU + GSM8K + HumanEval
- **Reasoning Suite**: ARC + HellaSwag + WinoGrande
- **Comprehensive Suite**: Best 8-benchmark coverage

## ğŸ”¥ Confirmed Working Local Models (23+)

### ï¿½ **Ollama Models (15 discovered)**
- **llama3.2** - Meta's latest compact models
- **qwen2.5** - Alibaba's optimized instruction models  
- **qwen2.5-coder** - Specialized coding variants
- **granite3.2** - IBM's enterprise-ready models
- **deepseek-r1** - Reasoning-focused models
- **gemma-3** - Google's efficient instruction models
- **liquid** - High-performance compact models
- **And 8+ more automatically discovered**

### ğŸ¯ **LM Studio Models (8 discovered)**  
- **Meta Llama variants** - Multiple sizes and versions
- **Qwen2.5 series** - Instruction and coder variants
- **Google Gemma models** - Various parameter sizes
- **Granite models** - IBM's latest offerings
- **DeepSeek variants** - Reasoning and general models

### â˜ï¸ **Cloud Models (HuggingFace)**
All models from the original cloud configuration still supported for comparison.

## ğŸ› ï¸ Technology Stack

### **Core Technologies**
- **Python 3.8+**: Modern async/await patterns for concurrent evaluation
- **HuggingFace Hub**: Direct API integration for model inference
- **Datasets Library**: Standardized benchmark data loading
- **AsyncIO**: Non-blocking concurrent model evaluation
- **YAML**: Human-readable configuration management

### **Data & Analytics**
- **Pandas**: Data manipulation and analysis
- **NumPy**: Numerical computing for metrics calculation
- **SciPy**: Statistical analysis and significance testing
- **Matplotlib/Seaborn**: Data visualization for reports

### **Web & Interface**
- **Gradio**: Optional web interface for interactive evaluation
- **FastAPI**: REST API for programmatic access
- **Beautiful Terminal**: Custom ANSI-colored terminal interface
- **HTML Export**: Static website generation from results

### **Evaluation Framework**
- **Custom Benchmarks**: Modular benchmark system
- **Async Model Manager**: Efficient model loading and inference
- **Result Aggregation**: Statistical analysis and ranking
- **Cost Estimation**: Real-time API cost tracking

## ğŸš€ Quick Start

### 1. **Installation**
```bash
git clone https://github.com/mmdmcy/smaLLMs.git
cd smaLLMs
pip install -r requirements.txt
```

### 2. **Setup Local Models (Optional)**
```bash
# Install Ollama (if you want local models)
# Windows: Download from https://ollama.ai
# Then pull some models:
ollama pull llama3.2
ollama pull qwen2.5:0.5b
ollama pull granite3.2:2b

# Or use LM Studio: Download from https://lmstudio.ai
```

### 3. **Configuration (Cloud models only)**
```bash
# Only needed if using cloud models
cp config/config.example.yaml config/config.yaml
# Add your HuggingFace token to config/config.yaml
```

### 4. **Run Marathon Mode**
```bash
python smaLLMs.py
```

**Marathon Mode Options:**
- **ğŸ”„ Local**: Auto-discover and evaluate all Ollama + LM Studio models
- **â˜ï¸ Cloud**: Evaluate HuggingFace models (requires config)
- **ğŸ“Š Choose Models**: Select specific models from 23+ discovered
- **ï¿½ Choose Benchmarks**: Pick from 18 benchmark suite options
- **ğŸƒâ€â™‚ï¸ Run ALL**: Overnight evaluation of everything!

### 5. **Export & Analysis**
```bash
python simple_exporter.py
```
Generate beautiful websites, leaderboards, and analysis reports from your Marathon Mode results.

## ğŸ“Š Marathon Mode Performance

| Setup | Models | Benchmarks | Samples | Duration | Use Case |
|-------|--------|------------|---------|----------|----------|
| **Quick Test** | 3 local | 2 core | 25 | ~15 min | Testing setup |
| **Standard** | 8 local | 4 suites | 50 | ~2 hours | Daily evaluation |
| **Comprehensive** | 15 local | 8 benchmarks | 100 | ~6 hours | Weekly analysis |
| **Marathon ALL** | 23 models | 16 benchmarks | 200 | ~12 hours | Complete evaluation |

*Local model evaluation is FREE - no API costs!*

## ğŸ¯ Confirmed Working Models

smaLLMs focuses on **reliability** with automatic model discovery:

### **Local Models (FREE)** âœ…
**Auto-discovered from Ollama & LM Studio:**
- ğŸ“ˆ **15 Ollama models** - Automatically detected and configured
- ğŸ¯ **8 LM Studio models** - Seamlessly integrated
- ğŸ”„ **Progressive timeouts** - Handles slower local inference
- ğŸ’¾ **Efficient caching** - Faster repeat evaluations

### **Cloud Models (API Required)** âœ…  
**Battle-tested HuggingFace models:**
- `google/gemma-2-2b-it` - Google's efficient instruction model
- `Qwen/Qwen2.5-1.5B-Instruct` - Alibaba's optimized model  
- `meta-llama/Llama-3.2-1B-Instruct` - Meta's compact model
- `HuggingFaceTB/SmolLM2-1.7B-Instruct` - HF's optimized model
- *Plus 6 more proven models*

*Marathon Mode automatically discovers your available models - no manual configuration needed!*

## ğŸ“ File Structure (Streamlined)

```
smaLLMs/
â”œâ”€â”€ ğŸ“„ smaLLMs.py              # ğŸƒâ€â™‚ï¸ Main Marathon Mode launcher (ALL-IN-ONE)
â”œâ”€â”€ ğŸ§  intelligent_evaluator.py # Smart evaluation engine
â”œâ”€â”€ ğŸ“Š simple_exporter.py      # Results export & website generation
â”œâ”€â”€ ğŸ¨ beautiful_terminal.py   # Color terminal interface
â”œâ”€â”€ ğŸ§ª test_everything.py      # Comprehensive test suite (15 tests)
â”œâ”€â”€ ğŸ” check_local_services.py # Local model discovery utility
â”œâ”€â”€ âš™ï¸ config/
â”‚   â”œâ”€â”€ config.yaml            # Your configuration (cloud only)
â”‚   â”œâ”€â”€ config.example.yaml    # Example configuration
â”‚   â””â”€â”€ models.yaml            # Model definitions
â”œâ”€â”€ ğŸ”§ src/                    # Core evaluation modules
â”‚   â”œâ”€â”€ models/                # Model management & discovery
â”‚   â”œâ”€â”€ benchmarks/            # 16 benchmark implementations
â”‚   â”œâ”€â”€ evaluator.py           # Evaluation orchestration
â”‚   â”œâ”€â”€ metrics/               # Result analysis & aggregation
â”‚   â”œâ”€â”€ utils/                 # Storage and utilities
â”‚   â””â”€â”€ web/                   # Optional web interface
â””â”€â”€ ğŸ“ smaLLMs_results/        # Marathon Mode results
    â””â”€â”€ 2025-MM-DD/            # Date-based organization
        â””â”€â”€ run_HHMMSS/        # Time-stamped runs
            â”œâ”€â”€ individual_results/ # Raw benchmark data
            â”œâ”€â”€ reports/           # Human-readable summaries
            â””â”€â”€ exports/           # Website/analysis exports
```

**Everything you need in 17 essential files - no bloat!**

## ğŸ› ï¸ How It Works

### **1. Marathon Mode Discovery**
```python
# Automatic model discovery across platforms
models = discover_local_models()  # Finds Ollama + LM Studio
benchmarks = load_benchmark_suite()  # All 16 AI studio benchmarks
```

### **2. Intelligent Orchestration**
- **Progressive Timeouts**: Adapts to local model inference speed
- **Smart Sampling**: Optimizes evaluation depth based on model performance  
- **Error Recovery**: Robust handling of model failures and timeouts
- **Progress Tracking**: Real-time updates with beautiful terminal interface

### **3. Local Model Integration**
- **Ollama API**: Direct integration with local Ollama models
- **LM Studio API**: Seamless connection to LM Studio inference server
- **Unified Interface**: Same benchmarks work across all model types
- **No API Costs**: Free evaluation of local models

### **4. Organized Data Management**
- **Auto Directory Creation**: Date/timestamp-based result organization
- **Multiple Formats**: JSON for machines, human-readable summaries
- **Export Ready**: One-click website and analysis generation
- **Resume Capability**: Continue interrupted Marathon Mode runs

## ğŸ’° Cost Optimization & FREE Local Evaluation

### **FREE Local Models** ğŸ‰
Marathon Mode with local models is **completely FREE**:
- **No API costs** for Ollama and LM Studio models
- **Unlimited evaluations** - run as many benchmarks as you want
- **23+ models available** - comprehensive local model comparison
- **Perfect for research** and experimentation

### **Cost-Efficient Cloud Models** ğŸ’°
When using cloud APIs, smaLLMs is optimized for efficiency:
- **Smart Sampling**: Don't waste tokens on failing models
- **Progressive Evaluation**: Start small, scale for promising models  
- **Rate Limiting**: Respect free tier limits
- **Early Stopping**: Skip models that consistently fail

**Typical cloud costs:**
- Quick test (3 models, 2 benchmarks): ~$0.05
- Standard evaluation (8 models, 4 benchmarks): ~$0.30
- Comprehensive (15 models, 8 benchmarks): ~$1.20

## ğŸŒ Export & Integration

### **Marathon Mode Results Export**
```bash
python simple_exporter.py
```
**Generates:**
- **ğŸ“Š Beautiful Websites**: Interactive leaderboards and analysis
- **ğŸ“ˆ Comparison Charts**: Visual model performance comparisons
- **ğŸ“‹ CSV/JSON Data**: Excel and analysis-ready formats
- **ğŸ“ Markdown Reports**: AI assistant and documentation ready
- **ğŸ† Leaderboards**: Rank your local models against benchmarks

### **Integration Ready**
- **REST API**: Optional web interface (via FastAPI)
- **JSON Data**: Machine-readable results for custom analysis
- **Modular Architecture**: Easy to extend with custom benchmarks
- **Plugin System**: Add new model providers and benchmarks

## ğŸ”§ Configuration

### **Local Models (No config needed!)**
Marathon Mode automatically discovers your models:
```bash
# Just run it - no configuration required!
python smaLLMs.py
```

### **Cloud Models (Optional)**
```yaml
# config/config.yaml (only if using cloud models)
huggingface:
  token: "your_hf_token_here"

evaluation:
  default_samples: 100
  max_concurrent_requests: 3
  
marathon_mode:
  local_models_enabled: true
  auto_discovery: true
  result_organization: "date_time"
```

### **Advanced Customization**
- **Custom Benchmark Configurations**: Modify sample sizes and parameters
- **Model-Specific Settings**: Timeout and generation configs per model
- **Result Organization**: Customize directory structure and naming
- **Export Formats**: Configure output formats and destinations

## ğŸš€ Get Started with Marathon Mode

```bash
git clone https://github.com/mmdmcy/smaLLMs.git
cd smaLLMs
pip install -r requirements.txt
python smaLLMs.py
```

**Choose your adventure:**
1. **ğŸ”„ Local Models**: Auto-discover and evaluate all Ollama + LM Studio models (FREE!)
2. **â˜ï¸ Cloud Models**: Add HuggingFace token and evaluate cloud models  
3. **ğŸƒâ€â™‚ï¸ Marathon Mode**: Run ALL models with ALL 16 benchmarks overnight
4. **ğŸ“Š Custom**: Pick specific models and benchmark suites

**Join the local model revolution!** ğŸ”¥

---

## ğŸ¤ Contributing

Help make smaLLMs even better:

- **ğŸ§ª New Benchmarks**: Add domain-specific evaluation tasks
- **ğŸ¤– Model Providers**: Integrate new local and cloud model platforms  
- **ğŸ“Š Visualization**: Enhance Marathon Mode result analysis
- **ï¿½ Performance**: Optimize local model inference and evaluation speed

## ğŸ“„ License

MIT License - see [LICENSE](LICENSE) for details.

## ğŸ‘¨â€ğŸ’» Created By

**mmdmcy** - [GitHub](https://github.com/mmdmcy)

*Building comprehensive local model evaluation with Marathon Mode - because your local models deserve AI studio-level benchmarking.*

---

*smaLLMs Marathon Mode - Run overnight evaluation of ALL your models with ALL benchmarks. Local is the new cloud.* ğŸƒâ€â™‚ï¸âœ¨
